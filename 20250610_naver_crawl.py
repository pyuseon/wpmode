from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime, timedelta
import csv
import time
import extract_factor_util as extract_util
import os
import logging


def setup_logging():
    """ë¡œê¹… ì„¤ì • í•¨ìˆ˜"""
    # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
    now = datetime.now()
    log_filename = f"logs/naver_crawl_{now.strftime('%y%m%d_%H%M')}.log"

    # logs ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("logs", exist_ok=True)

    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger('naver_crawler')
    logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ í•œê¸€ ì§€ì›)
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    # ë¡œê·¸ í¬ë§· ì„¤ì •
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger


class ImprovedRateLimiter:
    """ê°œì„ ëœ ìš”ì²­ ì†ë„ ì œí•œ í´ë˜ìŠ¤"""

    def __init__(self, max_requests=10, time_window=60, min_delay=2, logger=None):
        self.max_requests = max_requests  # ì‹œê°„ ìœˆë„ìš°ë‹¹ ìµœëŒ€ ìš”ì²­ ìˆ˜
        self.time_window = time_window  # ì‹œê°„ ìœˆë„ìš° (ì´ˆ)
        self.min_delay = min_delay  # ìš”ì²­ ê°„ ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        self.requests = []  # ìš”ì²­ ì‹œê°„ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
        self.last_request_time = 0  # ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê°„
        self.logger = logger or logging.getLogger('naver_crawler')

        # í†µê³„
        self.total_requests = 0
        self.total_wait_time = 0

    def wait_if_needed(self, request_type="ì¼ë°˜"):
        """í•„ìš”ì‹œ ëŒ€ê¸°í•˜ëŠ” ë©”ì„œë“œ"""
        current_time = time.time()
        self.total_requests += 1

        # 1. ìµœì†Œ ëŒ€ê¸° ì‹œê°„ ì²´í¬ (ì—°ì† ìš”ì²­ ë°©ì§€)
        time_since_last = current_time - self.last_request_time
        if time_since_last < self.min_delay:
            min_wait = self.min_delay - time_since_last
            self.logger.info(f"â±ï¸  ìµœì†Œ ëŒ€ê¸°: {min_wait:.1f}ì´ˆ ({request_type} ìš”ì²­)")
            time.sleep(min_wait)
            current_time = time.time()
            self.total_wait_time += min_wait

        # 2. ì‹œê°„ ìœˆë„ìš°ë¥¼ ë²—ì–´ë‚œ ì˜¤ë˜ëœ ìš”ì²­ë“¤ ì œê±°
        self.requests = [req_time for req_time in self.requests
                         if current_time - req_time < self.time_window]

        # 3. ìµœëŒ€ ìš”ì²­ ìˆ˜ ì²´í¬
        if len(self.requests) >= self.max_requests:
            oldest_request = min(self.requests)
            window_wait = self.time_window - (current_time - oldest_request)
            if window_wait > 0:
                self.logger.info(f"â° ìœˆë„ìš° ì œí•œ: {window_wait:.1f}ì´ˆ ëŒ€ê¸° ì¤‘... "
                                 f"({self.time_window}ì´ˆì— {self.max_requests}íšŒ ì œí•œ)")
                time.sleep(window_wait + 0.5)  # ì—¬ìœ  ì‹œê°„ ì¶”ê°€
                current_time = time.time()
                self.total_wait_time += window_wait + 0.5

        # 4. í˜„ì¬ ìš”ì²­ ì‹œê°„ ê¸°ë¡
        self.requests.append(current_time)
        self.last_request_time = current_time

        # 5. ì§„í–‰ ìƒí™© ë¡œê¹…
        if self.total_requests % 10 == 0:
            avg_wait = self.total_wait_time / self.total_requests
            self.logger.info(f"ğŸ“Š ìš”ì²­ í†µê³„: {self.total_requests}íšŒ ì™„ë£Œ, "
                             f"í‰ê·  ëŒ€ê¸°: {avg_wait:.1f}ì´ˆ")

    def get_stats(self):
        """í†µê³„ ë°˜í™˜"""
        return {
            'total_requests': self.total_requests,
            'total_wait_time': self.total_wait_time,
            'avg_wait_time': self.total_wait_time / max(self.total_requests, 1),
            'current_window_requests': len(self.requests)
        }


def debug_page_elements(driver, logger, wait_time=3):
    """í˜ì´ì§€ì˜ ëª¨ë“  ê°€ëŠ¥í•œ ë‰´ìŠ¤ ìš”ì†Œë“¤ì„ ì°¾ì•„ì„œ ì¶œë ¥í•˜ëŠ” ë””ë²„ê·¸ í•¨ìˆ˜"""
    time.sleep(wait_time)

    logger.info("=== ğŸ” í˜ì´ì§€ ë””ë²„ê¹… ì‹œì‘ ===")

    # 1. ì „ì²´ í˜ì´ì§€ êµ¬ì¡° í™•ì¸
    try:
        main_pack = driver.find_element(By.CSS_SELECTOR, "#main_pack")
        logger.info("âœ… #main_pack ì¡´ì¬")
    except:
        logger.info("âŒ #main_pack ì—†ìŒ")

    # 2. ë‰´ìŠ¤ ê´€ë ¨ divë“¤ ëª¨ë‘ ì°¾ê¸°
    possible_selectors = [
        "div[class*='news']",
        "div[class*='group']",
        "div[class*='api']",
        "div[class*='subject']",
        "div[class*='area']",
        "div[class*='wrap']",
        "div[class*='item']",
        "div[class*='card']",
        "div[class*='list']",
        "div[class*='vertical']",
        "div[class*='horizontal']"
    ]

    for selector in possible_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                logger.info(f"ğŸ“¦ {selector}: {len(elements)}ê°œ ë°œê²¬")
                # ì²« ë²ˆì§¸ ìš”ì†Œì˜ í´ë˜ìŠ¤ëª… ì¶œë ¥
                if elements[0].get_attribute("class"):
                    logger.info(f"    í´ë˜ìŠ¤: {elements[0].get_attribute('class')}")
        except:
            continue

    # 3. ë§í¬ ìš”ì†Œë“¤ ì°¾ê¸°
    logger.info("=== ğŸ“ ë§í¬ ìš”ì†Œ ë¶„ì„ ===")
    link_selectors = [
        "a[href*='news.naver.com']",
        "a[href*='n.news.naver.com']",
        "a[class*='news']",
        "a[class*='tit']",
        "a[class*='title']",
        "a[class*='headline']"
    ]

    for selector in link_selectors:
        try:
            links = driver.find_elements(By.CSS_SELECTOR, selector)
            if links:
                logger.info(f"ğŸ”— {selector}: {len(links)}ê°œ")
                if links[0].text.strip():
                    logger.info(f"    ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸: {links[0].text.strip()[:50]}...")
        except:
            continue

    # 4. ëª¨ë“  a íƒœê·¸ì˜ í´ë˜ìŠ¤ëª… ìˆ˜ì§‘
    logger.info("=== ğŸ·ï¸ ëª¨ë“  ë§í¬ í´ë˜ìŠ¤ëª… ë¶„ì„ ===")
    all_links = driver.find_elements(By.TAG_NAME, "a")
    class_names = set()
    for link in all_links[:20]:  # ì²˜ìŒ 20ê°œë§Œ
        if link.get_attribute("class"):
            class_names.add(link.get_attribute("class"))
            if link.text.strip() and len(link.text.strip()) > 10:
                logger.info(f"ğŸ“° í´ë˜ìŠ¤: {link.get_attribute('class')}")
                logger.info(f"    í…ìŠ¤íŠ¸: {link.text.strip()[:80]}...")
                logger.info(f"    href: {link.get_attribute('href')[:80] if link.get_attribute('href') else 'None'}...")
                logger.info("---")

    logger.info(f"ì´ {len(all_links)}ê°œì˜ ë§í¬ ë°œê²¬")
    return True


def crawl_with_intelligent_detection(keyword, start_date_str, end_date_str, logger):
    # ì†ë„ ì œí•œê¸° ì´ˆê¸°í™” (1ë¶„ì— 10íšŒ)
    rate_limiter = ImprovedRateLimiter(max_requests=10, time_window=60, logger=logger)

    # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    now = datetime.now()
    start_str = now.strftime("%y%m%d")  # yymmdd í˜•ì‹
    time_str = now.strftime("%H%M")  # hhmm í˜•ì‹
    output_filename = f"naver_news_{keyword}_{start_str}_{time_str}_({start_date_str}to{end_date_str}).csv"

    options = webdriver.ChromeOptions()
    options.add_argument("--window-size=1200,900")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_experimental_option("detach", True)

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    results = []
    start_date = datetime.strptime(start_date_str, "%Y%m%d")
    end_date = datetime.strptime(end_date_str, "%Y%m%d")

    try:
        current_date = start_date
        while current_date <= end_date:
            start_str = current_date.strftime("%Y%m%d")
            logger.info("================================")

            # 7ì¼ ë‹¨ìœ„ë¡œ ë‚ ì§œ ë¬¸ìì—´ ìƒì„±
            current_plus_6day = current_date + timedelta(days=6)

            if current_plus_6day <= end_date:
                end_str = (current_date + timedelta(days=6)).strftime("%Y%m%d")
            else:
                end_str = end_date.strftime("%Y%m%d")

            page = 1

            logger.info(f"ğŸ“„ {start_str}ë¶€í„° {end_str}ê¹Œì§€ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

            finished = False

            while True:
                if finished:
                    logger.info(f"âœ… {start_str} to {end_str} ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                    break

                # ì†ë„ ì œí•œ ì ìš©
                rate_limiter.wait_if_needed()

                start_num = (page - 1) * 10 + 1

                office_category = "3"  # 1: ì¼ê°„ì§€, 2: ë°©ì†¡í†µì‹ , 3: ê²½ì œIT, 4: ì¸í„°ë„·ì‹ ë¬¸
                url = f"https://search.naver.com/search.naver?where=news&query={keyword}&nso=so:r,p:from{start_str}to{end_str},a:all&start={start_num}&service_area=1&office_category={office_category}"
                logger.info(f"ğŸŒ ì ‘ì† URL: {url}")
                driver.get(url)

                # ì²« í˜ì´ì§€ì—ì„œë§Œ ë””ë²„ê¹… ì‹¤í–‰
                if current_date == start_date and page == 1:
                    debug_page_elements(driver, logger)
                else:
                    time.sleep(3)

                # ì§€ëŠ¥ì ìœ¼ë¡œ ë‰´ìŠ¤ ì¹´ë“œ ì°¾ê¸°
                logger.info(f"=== ğŸ¯ ë‰´ìŠ¤ ì¹´ë“œ íƒì§€ ì‹œì‘ (í˜ì´ì§€ {page}) ===")
                news_cards = extract_util.find_elements_intelligently(driver)

                if not news_cards:
                    logger.info(f"âŒ ë” ì´ìƒ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤ (í˜ì´ì§€ {page})")
                    break

                logger.info(f"âœ… {len(news_cards)}ê°œì˜ ë‰´ìŠ¤ ì¹´ë“œ ë°œê²¬!")

                if len(news_cards) < 8:
                    finished = True

                # ê° ì¹´ë“œì—ì„œ ì •ë³´ ì¶”ì¶œ
                for i, card in enumerate(news_cards):
                    logger.info(f"--- ğŸ“° ë‰´ìŠ¤ {i + 1} ì²˜ë¦¬ ì¤‘ (í˜ì´ì§€ {page}) ---")

                    try:
                        # ì œëª© ì¶”ì¶œ
                        title = extract_util.extract_title_intelligently(card)
                        if not title:
                            logger.warning(f"    âŒ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨")
                            continue

                        # URL ì¶”ì¶œ
                        naver_url = extract_util.extract_naver_url(card)
                        original_url = extract_util.extract_original_url(card)
                        logger.info(f"        ë„¤ì´ë²„ URL: {naver_url}")
                        logger.info(f"        ì›ë³¸ URL: {original_url}")

                        # ì–¸ë¡ ì‚¬ ì¶”ì¶œ (ê°„ë‹¨íˆ)
                        press = extract_util.extract_press(card)
                        # ë°œí–‰ì¼ ì¶”ì¶œ
                        published = extract_util.extract_published(card)
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                        image_url = extract_util.extract_img_url(card)

                        result = {
                            "title": title,
                            "naver_url": naver_url,
                            "original_url": original_url,
                            "source": press,
                            "published": published,
                            "image_url": image_url,
                            "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "scraped_url": url
                        }

                        results.append(result)
                        logger.info(f"    âœ… ì¶”ì¶œ ì™„ë£Œ: {title[:30]}... | {press} | {published}")

                    except Exception as e:
                        logger.error(f"    âŒ ë‰´ìŠ¤ {i + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue

                logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì™„ë£Œ")
                time.sleep(2)
                page += 1

            logger.info(f"ğŸ“… {start_str} to {end_str} ìˆ˜ì§‘ ì™„ë£Œ")

            current_date = datetime.strptime(end_str, "%Y%m%d") + timedelta(days=1)

        save_results(output_filename, results, logger)

        # ìµœì¢… í†µê³„ ì¶œë ¥
        stats = rate_limiter.get_stats()
        logger.info(f"ğŸ“Š ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ìµœì¢… í†µê³„:")
        logger.info(f"   ì´ ìš”ì²­: {stats['total_requests']}íšŒ")
        logger.info(f"   ì´ ëŒ€ê¸° ì‹œê°„: {stats['total_wait_time']:.1f}ì´ˆ")
        logger.info(f"   í‰ê·  ëŒ€ê¸° ì‹œê°„: {stats['avg_wait_time']:.1f}ì´ˆ")

    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    finally:
        driver.quit()

    return results


def save_results(output_filename, results, logger):
    # ê²°ê³¼ ì €ì¥
    if results:
        output_directory = "results/"
        os.makedirs(output_directory, exist_ok=True)

        # ID ì¶”ê°€ ë° ì¤‘ë³µ ì œê±°
        processed_results = []
        seen_urls = set()  # ì¤‘ë³µ ì²´í¬ìš© (URL ê¸°ì¤€)

        for result in results:  # enumerate ì œê±°
            # URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (naver_url ë˜ëŠ” original_url)
            url_key = result.get('naver_url') or result.get('original_url')
            if url_key and url_key in seen_urls:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ë°œê²¬: {url_key} (ê±´ë„ˆëœ€)")
                logger.info(f"    ì œëª©: {result.get('title')}")
                continue  # ì¤‘ë³µì´ë©´ ê±´ë„ˆë›°ê¸°

            if url_key:
                seen_urls.add(url_key)

            # published ì—¬ë¶€ ì²´í¬
            result['has_published'] = 'Y' if result.get('published') else 'N'

            processed_results.append(result)

        # ì¤‘ë³µ ì œê±° í›„ ID ë¶€ì—¬
        for idx, result in enumerate(processed_results, 1):
            result['id'] = idx

        filepath = os.path.join(output_directory, output_filename)

        try:
            with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.DictWriter(f, fieldnames=[
                    "id", "title", "naver_url", "original_url", "source",
                    "published", "has_published", "image_url", "scraped_at", "scraped_url"
                ])
                writer.writeheader()
                writer.writerows(processed_results)

            # ê²°ê³¼ ìš”ì•½
            original_count = len(results)
            final_count = len(processed_results)
            duplicate_count = original_count - final_count
            no_published_count = sum(1 for r in processed_results if r['has_published'] == 'N')

            logger.info(f"   ğŸ“° ì „ì²´ ë‰´ìŠ¤: {original_count}ê±´")
            logger.info(f"   ğŸ”„ ì¤‘ë³µ ì œê±°: {duplicate_count}ê±´")
            logger.info(f"   âœ… ìµœì¢… ì €ì¥: {final_count}ê±´")
            logger.info(f"   âš ï¸  ë°œí–‰ì¼ ì—†ìŒ: {no_published_count}ê±´")
            logger.info(f"   ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {filepath}")

        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    else:
        logger.warning("âŒ ì¶”ì¶œëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")


def format_duration(seconds):
    """ì´ˆë¥¼ ì‹œ:ë¶„:ì´ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60

    if hours > 0:
        return f"{hours}ì‹œê°„ {minutes}ë¶„ {secs:.1f}ì´ˆ"
    elif minutes > 0:
        return f"{minutes}ë¶„ {secs:.1f}ì´ˆ"
    else:
        return f"{secs:.2f}ì´ˆ"


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging()

    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    start_datetime = datetime.now()

    logger.info("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘")
    logger.info(f"â° ì‹œì‘ ì‹œê°„: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("-" * 50)

    results = crawl_with_intelligent_detection("\"ìœ¡ì•„íœ´ì§\"", "20240601", "20250531", logger)

    # crawl_target_dates = ["20240701", "20240901", "20241201", "20250201"]
    # keyword = "\"ì•„ë™\""
    # all_results = []
    #
    # for i in range(len(crawl_target_dates)):
    #     start_date = crawl_target_dates[i]
    #     if i < len(crawl_target_dates) - 1:
    #         end_date = datetime.strptime(crawl_target_dates[i + 1], "%Y%m%d") + timedelta(days=-1)
    #         end_date = end_date.strftime("%Y%m%d")
    #     else:
    #         end_date = "20250531"
    #     logger.info(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {start_date} ~ {end_date}")
    #     results = crawl_with_intelligent_detection(keyword, start_date, end_date, logger)
    #     all_results.extend(results)

    # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    end_time = time.time()
    end_datetime = datetime.now()
    execution_time = end_time - start_time

    logger.info("-" * 50)
    logger.info(f"â° ì¢…ë£Œ ì‹œê°„: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"âŒ› ì´ ì‹¤í–‰ ì‹œê°„: {format_duration(execution_time)}")

    # if all_results:
    #     logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
    #     logger.info(f"ğŸ“ˆ ì´ ë‰´ìŠ¤: {len(all_results)}ê±´")
    #     logger.info(f"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(all_results) / execution_time:.2f}ê±´/ì´ˆ")
    # else:
    #     logger.warning("ğŸ”§ í˜„ì¬ í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    #     logger.warning("   ìœ„ì˜ ë””ë²„ê¹… ì¶œë ¥ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ ì…€ë ‰í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")