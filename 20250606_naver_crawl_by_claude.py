from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime, timedelta
import csv
import time
import re


def debug_page_elements(driver, wait_time=3):
    """í˜ì´ì§€ì˜ ëª¨ë“  ê°€ëŠ¥í•œ ë‰´ìŠ¤ ìš”ì†Œë“¤ì„ ì°¾ì•„ì„œ ì¶œë ¥í•˜ëŠ” ë””ë²„ê·¸ í•¨ìˆ˜"""
    time.sleep(wait_time)

    print("\n=== ğŸ” í˜ì´ì§€ ë””ë²„ê¹… ì‹œì‘ ===")

    # 1. ì „ì²´ í˜ì´ì§€ êµ¬ì¡° í™•ì¸
    try:
        main_pack = driver.find_element(By.CSS_SELECTOR, "#main_pack")
        print("âœ… #main_pack ì¡´ì¬")
    except:
        print("âŒ #main_pack ì—†ìŒ")

    # 2. ë‰´ìŠ¤ ê´€ë ¨ divë“¤ ëª¨ë‘ ì°¾ê¸°
    possible_selectors = [
        "div[class*='news']",
        "div[class*='group']",
        "div[class*='api']",
        "div[class*='subject']",
        "div[class*='area']",
        "div[class*='wrap']",
        "div[class*='item']",
        "div[class*='card']",
        "div[class*='list']",
        "div[class*='vertical']",
        "div[class*='horizontal']"
    ]

    for selector in possible_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"ğŸ“¦ {selector}: {len(elements)}ê°œ ë°œê²¬")
                # ì²« ë²ˆì§¸ ìš”ì†Œì˜ í´ë˜ìŠ¤ëª… ì¶œë ¥
                if elements[0].get_attribute("class"):
                    print(f"    í´ë˜ìŠ¤: {elements[0].get_attribute('class')}")
        except:
            continue

    # 3. ë§í¬ ìš”ì†Œë“¤ ì°¾ê¸°
    print("\n=== ğŸ“ ë§í¬ ìš”ì†Œ ë¶„ì„ ===")
    link_selectors = [
        "a[href*='news.naver.com']",
        "a[href*='n.news.naver.com']",
        "a[class*='news']",
        "a[class*='tit']",
        "a[class*='title']",
        "a[class*='headline']"
    ]

    for selector in link_selectors:
        try:
            links = driver.find_elements(By.CSS_SELECTOR, selector)
            if links:
                print(f"ğŸ”— {selector}: {len(links)}ê°œ")
                if links[0].text.strip():
                    print(f"    ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸: {links[0].text.strip()[:50]}...")
        except:
            continue

    # 4. ëª¨ë“  a íƒœê·¸ì˜ í´ë˜ìŠ¤ëª… ìˆ˜ì§‘
    print("\n=== ğŸ·ï¸ ëª¨ë“  ë§í¬ í´ë˜ìŠ¤ëª… ë¶„ì„ ===")
    all_links = driver.find_elements(By.TAG_NAME, "a")
    class_names = set()
    for link in all_links[:20]:  # ì²˜ìŒ 20ê°œë§Œ
        if link.get_attribute("class"):
            class_names.add(link.get_attribute("class"))
            if link.text.strip() and len(link.text.strip()) > 10:
                print(f"ğŸ“° í´ë˜ìŠ¤: {link.get_attribute('class')}")
                print(f"    í…ìŠ¤íŠ¸: {link.text.strip()[:80]}...")
                print(f"    href: {link.get_attribute('href')[:80] if link.get_attribute('href') else 'None'}...")
                print("---")

    print(f"\nì´ {len(all_links)}ê°œì˜ ë§í¬ ë°œê²¬")
    return True


def find_elements_intelligently(driver, base_element=None):
    """ì§€ëŠ¥ì ìœ¼ë¡œ ë‰´ìŠ¤ ìš”ì†Œë“¤ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    search_base = base_element if base_element else driver

    # 1ë‹¨ê³„: ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ê¸° ìœ„í•œ íŒ¨í„´ë“¤
    container_patterns = [
        # ìµœì‹  ì»¨í…Œì´ë„ˆ íŒ¨í„´ë“¤
        "div.sds-comps-vertical-layout.sds-comps-full-layout.fender-news-item-list-tab"
        "div.fender-news-item-list-tab",
        "div[class*='fender-news-item-list']",
        # "div[class*='news-item-list']",
        # "div.group_news",
        # "div[class*='api_subject']",
        # "#main_pack"
    ]

    container = None
    for pattern in container_patterns:
        try:
            found_container = search_base.find_element(By.CSS_SELECTOR, pattern)
            if found_container:
                container = found_container
                print(f"ğŸ¯ ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ë°œê²¬: {pattern}")
                break
        except:
            continue

    if not container:
        print("âŒ ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return []

        # 2ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ê°œë³„ ë‰´ìŠ¤ ì¹´ë“œë“¤ ì°¾ê¸°
    card_patterns = [
        # ì •í™•í•œ ê°œë³„ ì¹´ë“œ íŒ¨í„´ (2024ë…„ 6ì›”)
        "div.sds-comps-vertical-layout.sds-comps-full-layout._4zQ0QZWfn7bqZ_ul5OV",
        "div[class*='sds-comps-vertical-layout'][class*='sds-comps-full-layout'][class*='_4zQ0QZWfn7bqZ_ul5OV']",
        "div[class*='_4zQ0QZWfn7bqZ_ul5OV']",  # ê³ ìœ  í´ë˜ìŠ¤ë¡œ ì°¾ê¸°

        # ì¢€ ë” ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ (ë°±ì—…ìš©)
        "div.sds-comps-vertical-layout.sds-comps-full-layout:not(.fender-news-item-list-tab)",
        "div[class*='sds-comps-vertical-layout'][class*='full-layout']:not([class*='fender'])",
        "div[class*='vertical-layout']:not([class*='fender']):not([class*='tab'])",
    ]

    found_cards = []
    for pattern in card_patterns:
        try:
            elements = container.find_elements(By.CSS_SELECTOR, pattern)
            if elements and len(elements) > 1:  # ì—¬ëŸ¬ ê°œì˜ ì¹´ë“œê°€ ìˆì–´ì•¼ í•¨
                print(f"ğŸ¯ ê°œë³„ ë‰´ìŠ¤ ì¹´ë“œ íŒ¨í„´ '{pattern}'ë¡œ {len(elements)}ê°œ ë°œê²¬")
                found_cards = elements
                break
            elif elements and len(elements) == 1:
                print(f"âš ï¸ íŒ¨í„´ '{pattern}'ë¡œ 1ê°œë§Œ ë°œê²¬ - ë‹¤ìŒ íŒ¨í„´ ì‹œë„")
        except Exception as e:
            print(f"âŒ íŒ¨í„´ '{pattern}' ì‹œë„ ì‹¤íŒ¨: {e}")
            continue

    # 3ë‹¨ê³„: ì—¬ì „íˆ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë” í¬ê´„ì ìœ¼ë¡œ ì°¾ê¸°
    if not found_cards:
        print("ğŸ” í¬ê´„ì  ê²€ìƒ‰ ì‹œì‘...")
        try:
            # ì»¨í…Œì´ë„ˆì˜ ëª¨ë“  ì§ì ‘ ìì‹ë“¤ ì¤‘ì—ì„œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒë“¤ ì°¾ê¸°
            all_children = container.find_elements(By.XPATH, "./*")
            potential_cards = []

            for child in all_children:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•˜ê³  ë§í¬ê°€ ìˆëŠ” ìš”ì†Œë“¤ë§Œ ì„ íƒ
                if child.text.strip() and len(child.text.strip()) > 10:
                    links = child.find_elements(By.CSS_SELECTOR, "a")
                    if links:  # ë§í¬ê°€ ìˆìœ¼ë©´ ë‰´ìŠ¤ ì¹´ë“œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        potential_cards.append(child)

            if potential_cards:
                print(f"ğŸ¯ í¬ê´„ì  ê²€ìƒ‰ìœ¼ë¡œ {len(potential_cards)}ê°œ ì¹´ë“œ ë°œê²¬")
                found_cards = potential_cards
        except Exception as e:
            print(f"âŒ í¬ê´„ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    return found_cards




def find_news_cards_in_container(container):


        # 2ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ê°œë³„ ë‰´ìŠ¤ ì¹´ë“œë“¤ ì°¾ê¸°
    card_patterns = [
        # ì •í™•í•œ ê°œë³„ ì¹´ë“œ íŒ¨í„´ (2024ë…„ 6ì›”)
        "div.sds-comps-vertical-layout.sds-comps-full-layout._4zQ0QZWfn7bqZ_ul5OV",
        "div[class*='sds-comps-vertical-layout'][class*='sds-comps-full-layout'][class*='_4zQ0QZWfn7bqZ_ul5OV']",
        "div[class*='_4zQ0QZWfn7bqZ_ul5OV']",  # ê³ ìœ  í´ë˜ìŠ¤ë¡œ ì°¾ê¸°

        # ì¢€ ë” ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ (ë°±ì—…ìš©)
        "div.sds-comps-vertical-layout.sds-comps-full-layout:not(.fender-news-item-list-tab)",
        "div[class*='sds-comps-vertical-layout'][class*='full-layout']:not([class*='fender'])",
        "div[class*='vertical-layout']:not([class*='fender']):not([class*='tab'])",
    ]

    found_cards = []
    for pattern in card_patterns:
        try:
            elements = container.find_elements(By.CSS_SELECTOR, pattern)
            if elements and len(elements) > 1:  # ì—¬ëŸ¬ ê°œì˜ ì¹´ë“œê°€ ìˆì–´ì•¼ í•¨
                print(f"ğŸ¯ ê°œë³„ ë‰´ìŠ¤ ì¹´ë“œ íŒ¨í„´ '{pattern}'ë¡œ {len(elements)}ê°œ ë°œê²¬")
                found_cards = elements
                break
            elif elements and len(elements) == 1:
                print(f"âš ï¸ íŒ¨í„´ '{pattern}'ë¡œ 1ê°œë§Œ ë°œê²¬ - ë‹¤ìŒ íŒ¨í„´ ì‹œë„")
        except Exception as e:
            print(f"âŒ íŒ¨í„´ '{pattern}' ì‹œë„ ì‹¤íŒ¨: {e}")
            continue

    # 3ë‹¨ê³„: ì—¬ì „íˆ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë” í¬ê´„ì ìœ¼ë¡œ ì°¾ê¸°
    if not found_cards:
        print("ğŸ” í¬ê´„ì  ê²€ìƒ‰ ì‹œì‘...")
        try:
            # ì»¨í…Œì´ë„ˆì˜ ëª¨ë“  ì§ì ‘ ìì‹ë“¤ ì¤‘ì—ì„œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒë“¤ ì°¾ê¸°
            all_children = container.find_elements(By.XPATH, "./*")
            potential_cards = []

            for child in all_children:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„í•˜ê³  ë§í¬ê°€ ìˆëŠ” ìš”ì†Œë“¤ë§Œ ì„ íƒ
                if child.text.strip() and len(child.text.strip()) > 10:
                    links = child.find_elements(By.CSS_SELECTOR, "a")
                    if links:  # ë§í¬ê°€ ìˆìœ¼ë©´ ë‰´ìŠ¤ ì¹´ë“œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        potential_cards.append(child)

            if potential_cards:
                print(f"ğŸ¯ í¬ê´„ì  ê²€ìƒ‰ìœ¼ë¡œ {len(potential_cards)}ê°œ ì¹´ë“œ ë°œê²¬")
                found_cards = potential_cards
        except Exception as e:
            print(f"âŒ í¬ê´„ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    return found_cards


def extract_title_intelligently(card):
    """ì§€ëŠ¥ì ìœ¼ë¡œ ì œëª©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    title_patterns = [
        # ì •í™•í•œ ìµœì‹  íŒ¨í„´ (2024ë…„ 6ì›”)
        "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1.sds-comps-text-type-headline1",
        "span[class*='sds-comps-text-type-headline1']",
        "span[class*='sds-comps-text'][class*='headline1']",
    ]

    for pattern in title_patterns:
        try:
            elements = card.find_elements(By.CSS_SELECTOR, pattern)
            for element in elements:
                text = element.text.strip()
                if text and len(text) > 5:  # ìµœì†Œ 5ê¸€ì ì´ìƒ
                    print(f"    ğŸ“° ì œëª© ë°œê²¬ (íŒ¨í„´: {pattern}): {text[:50]}...")
                    return text
        except:
            continue

    return ""


def find_expansion_link_from_spans(card):
    """ì¹´ë“œì—ì„œ ë”ë³´ê¸° ë§í¬ë¥¼ ì°¾ëŠ” í•¨ìˆ˜"""
    expansion_patterns = [
        "span.sds-comps-text.sds-comps-text-type-body2.sds-comps-text-weight-sm",
        "span[class*='sds-comps-text-type-body2'][class*='sds-comps-text-weight-sm']",
        "span[class*='sds-comps-text'][class*='body2'][class*='weight-sm']",
        "span.sds-comps-text-type-body2",
        "span.sds-comps-text-weight-sm",
    ]

    for pattern in expansion_patterns:
        try:
            span_elements = card.find_elements(By.CSS_SELECTOR, pattern)

            for i, span_elem in enumerate(span_elements):
                text = span_elem.text.strip() if span_elem.text else ""


                # í…ìŠ¤íŠ¸ ì¡°ê±´ í™•ì¸
                if "ê´€ë ¨ë‰´ìŠ¤" in text and "ì „ì²´ë³´ê¸°" in text:
                    try:
                        # ë¶€ëª¨ ìš”ì†Œë“¤ì„ í™•ì¸í•´ì„œ a íƒœê·¸ ì°¾ê¸°
                        parent = span_elem.find_element(By.XPATH, "..")
                        while parent and parent.tag_name.lower() != 'a':
                            parent = parent.find_element(By.XPATH, "..")

                        if parent and parent.tag_name.lower() == 'a':
                            href = parent.get_attribute("href")
                            if href:
                                print(f"      ğŸ¯ ë”ë³´ê¸° ë§í¬ ë°œê²¬!")
                                return href
                    except:
                        continue

        except Exception as e:
            continue

    print("      âš ï¸ ë”ë³´ê¸° ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    return ""


def process_related_news(driver, card, main_title, main_original_url, results):
    """ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë”ë³´ê¸° í˜ì´ì§€ + ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤)"""
    related_added = 0

    expansion_link = find_expansion_link_from_spans(card)

    if not expansion_link:
        print("      âš ï¸ ë”ë³´ê¸° ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

    # 2. ë”ë³´ê¸° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
    if expansion_link:
        try:
            print(f"      ğŸ“„ ë”ë³´ê¸° í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...")
            related_added += crawl_expansion_page(driver, expansion_link, main_title, main_original_url, results)
        except Exception as e:
            print(f"      âŒ ë”ë³´ê¸° í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    # 3. ë‚´ë¶€ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬ (ë”ë³´ê¸°ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°)
    if not expansion_link or related_added == 0:
        try:
            print(f"      ğŸ“° ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤ íƒì§€ ì¤‘...")
            related_added += crawl_internal_related_news(card, main_title, main_original_url, results)
        except Exception as e:
            print(f"      âŒ ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    return related_added


def find_original_link(card):
    """ì˜¤ë¦¬ì§€ë„ ë‰´ìŠ¤ ë§í¬ë¥¼ ì°¾ëŠ” í•¨ìˆ˜ - media.naver.com ì œì™¸"""

    # ë°©ë²• 1: nocr='1' ì†ì„±ì„ ê°€ì§„ ëª¨ë“  ë§í¬ ì°¾ê¸°
    try:
        links = card.find_elements(By.CSS_SELECTOR, "a[nocr='1']")
        print(f"      ğŸ” nocr='1' ë§í¬ {len(links)}ê°œ ë°œê²¬")

        for i, link in enumerate(links):
            href = link.get_attribute("href")
            text = link.text.strip() if link.text else ""
            print(f"         [{i + 1}] URL: {href}")

            # media.naver.com ì œì™¸í•˜ê³  ìœ íš¨í•œ ë§í¬ ì²´í¬
            if href and href.startswith('http') and "media.naver.com" not in href:
                print(f"      ğŸ¯ ì˜¤ë¦¬ì§€ë„ ë§í¬ ì„ íƒ: {href}")
                return href
            elif href and "media.naver.com" in href:
                print(f"         [{i + 1}] âŒ media.naver.com ë§í¬ ì œì™¸")

    except Exception as e:
        print(f"      âŒ nocr='1' ë§í¬ ì°¾ê¸° ì‹¤íŒ¨: {e}")

    # ë°©ë²• 2: sds-comps-vertical-layout ì»¨í…Œì´ë„ˆ ì•ˆì˜ ë§í¬
    try:
        containers = card.find_elements(By.CSS_SELECTOR, "div[class*='sds-comps-vertical-layout']")
        print(f"      ğŸ” vertical-layout ì»¨í…Œì´ë„ˆ {len(containers)}ê°œ ë°œê²¬")

        for container in containers:
            links = container.find_elements(By.CSS_SELECTOR, "a[href]")
            for link in links:
                href = link.get_attribute("href")
                if href and href.startswith('http') and "media.naver.com" not in href:
                    print(f"      ğŸ¯ ì»¨í…Œì´ë„ˆì—ì„œ ë§í¬ ë°œê²¬: {href}")
                    return href

    except Exception as e:
        print(f"      âŒ ì»¨í…Œì´ë„ˆ ë§í¬ ì°¾ê¸° ì‹¤íŒ¨: {e}")

    # ë°©ë²• 3: ì¹´ë“œ ë‚´ ëª¨ë“  ë§í¬ ì¤‘ì—ì„œ ì„ íƒ
    try:
        all_links = card.find_elements(By.CSS_SELECTOR, "a[href]")
        print(f"      ğŸ” ì „ì²´ ë§í¬ {len(all_links)}ê°œ ë°œê²¬")

        exclude_patterns = ['search', 'cluster', 'google', 'media.naver.com']
        valid_links = []

        for link in all_links:
            href = link.get_attribute("href")
            if href and href.startswith('http'):
                is_excluded = any(pattern in href for pattern in exclude_patterns)
                if not is_excluded:
                    valid_links.append(href)
                else:
                    print(f"         âŒ ì œì™¸ëœ ë§í¬: {href}")

        if valid_links:
            print(f"      ğŸ¯ ìœ íš¨í•œ ë§í¬ ì„ íƒ: {valid_links[0]}")
            return valid_links[0]

    except Exception as e:
        print(f"      âŒ ì „ì²´ ë§í¬ ì°¾ê¸° ì‹¤íŒ¨: {e}")

    print("      âš ï¸ ì˜¤ë¦¬ì§€ë„ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    return ""



def crawl_expansion_page(driver, expansion_link, main_title, main_original_url, results):
    """ë”ë³´ê¸° í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""
    added_count = 0
    original_window = driver.current_window_handle

    try:
        # ìƒˆ íƒ­ ì—´ê¸°
        driver.execute_script("window.open('');")
        driver.switch_to.window(driver.window_handles[1])

        start_page = 1
        max_pages = 5  # ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€ë§Œ

        while start_page <= max_pages * 10:
            paginated_url = f"{expansion_link}&start={start_page}"
            driver.get(paginated_url)
            time.sleep(2)

            # í™•ì¥ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            container_patterns = [
                "div.sds-comps-vertical-layout.sds-comps-full-layout.fender-news-item-list-tab",
                "div.fender-news-item-list-tab",
                "div[class*='fender-news-item-list']",
                "div[class*='news-item-list']",
            ]

            container = None
            for pattern in container_patterns:
                try:
                    container = driver.find_element(By.CSS_SELECTOR, pattern)
                    break
                except:
                    continue

            if not container:
                print(f"        âŒ í™•ì¥ í˜ì´ì§€ ì»¨í…Œì´ë„ˆ ì—†ìŒ")
                break

            # í™•ì¥ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ì¹´ë“œë“¤ ì°¾ê¸°
            expanded_cards = find_news_cards_in_container(container)

            if not expanded_cards or len(expanded_cards) <= 1:
                print(f"        â›” ë” ì´ìƒ ê´€ë ¨ë‰´ìŠ¤ ì—†ìŒ (í˜ì´ì§€ {start_page // 10 + 1})")
                break

            page_added = 0
            for i, related_card in enumerate(expanded_cards):
                if i == 0 and page_added == 0:  # ì²« ë²ˆì§¸ëŠ” ì›ë³¸ ë‰´ìŠ¤ì´ë¯€ë¡œ ìŠ¤í‚µ
                    continue

                try:
                    # ê´€ë ¨ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
                    related_title = extract_title_intelligently(related_card)
                    if not related_title :
                        continue

                    # URL ì¶”ì¶œ
                    related_naver_url = get_naver_url(related_card)
                    related_original_url = get_original_url(related_card)


                    print(f"        ë„¤ì´ë²„ URL: {related_naver_url}")
                    print(f"        ì›ë³¸ URL: {related_original_url}")

                    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                    related_press = ""
                    press_patterns = [
                        "span[class*='profile'][class*='title']",
                        "span[class*='press']",
                        "span[class*='info']"
                    ]
                    for pattern in press_patterns:
                        try:
                            press_elem = related_card.find_element(By.CSS_SELECTOR, pattern)
                            related_press = press_elem.text.strip()
                            if related_press:
                                break
                        except:
                            continue

                    # ë°œí–‰ì¼ ì¶”ì¶œ
                    related_published = ""
                    date_patterns = [
                        "span[class*='weight-sm']",
                        "span[class*='date']",
                        "span[class*='time']"
                    ]
                    for pattern in date_patterns:
                        try:
                            date_elem = related_card.find_element(By.CSS_SELECTOR, pattern)
                            text = date_elem.text.strip()
                            if any(x in text for x in ['ì‹œê°„', 'ë¶„', 'ì¼', 'ì›”', 'ë…„', ':']):
                                related_published = text
                                break
                        except:
                            continue

                    # ê²°ê³¼ì— ì¶”ê°€
                    results.append({
                        "title": related_title,
                        "naver_url": related_naver_url,
                        "original_url": related_original_url,
                        "source": related_press,
                        "published": related_published,
                        "related_to": main_title,
                        "related_original_url": main_original_url
                    })

                    added_count += 1
                    page_added += 1
                    print(f"        âœ… ê´€ë ¨ë‰´ìŠ¤ ì¶”ê°€: {related_title[:30]}...")

                except Exception as e:
                    print(f"        âŒ ê´€ë ¨ë‰´ìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue

            print(f"        ğŸ“„ í˜ì´ì§€ {start_page // 10 + 1}: {page_added}ê±´ ì¶”ê°€")

            if page_added == 0:  # ì´ í˜ì´ì§€ì—ì„œ ì•„ë¬´ê²ƒë„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìœ¼ë©´ ì¢…ë£Œ
                break

            start_page += 10

    except Exception as e:
        print(f"        âŒ í™•ì¥ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    finally:
        # ì›ë˜ íƒ­ìœ¼ë¡œ ëŒì•„ê°€ê¸°
        try:
            driver.close()  # í˜„ì¬ íƒ­ ë‹«ê¸°
            driver.switch_to.window(original_window)  # ì›ë˜ íƒ­ìœ¼ë¡œ
        except:
            pass

    return added_count


def crawl_internal_related_news(card, main_title, main_original_url, results):
    """ì¹´ë“œ ë‚´ë¶€ì˜ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""
    added_count = 0

    # ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
    related_container_patterns = [
        "div.sds-comps-vertical-layout.sds-comps-full-layout",  # ìƒˆë¡œìš´ íŒ¨í„´
        "div[class*='sds-comps-vertical-layout'][class*='sds-comps-full-layout']",
        "div[class*='sds-comps-vertical-layout']",
        "div[class*='sds-comps-full-layout']",
    ]

    related_container = None
    for pattern in related_container_patterns:
        try:
            related_container = card.find_element(By.CSS_SELECTOR, pattern)
            print(f"        ğŸ¯ ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ë°œê²¬: {pattern}")
            break
        except:
            continue

    if not related_container:
        print(f"        âŒ ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ì—†ìŒ")
        return 0

    # ê´€ë ¨ë‰´ìŠ¤ ì•„ì´í…œë“¤ ì°¾ê¸°
    related_item_patterns = [
        "div.sds-comps-base-layout.sds-comps-full-layout",  # ìƒˆë¡œìš´ íŒ¨í„´
        "div[class*='sds-comps-base-layout'][class*='sds-comps-full-layout']",
        "div[class*='sds-comps-base-layout']",
        "div[class*='sds-comps-full-layout']",
    ]

    related_items = []
    for pattern in related_item_patterns:
        try:
            items = related_container.find_elements(By.CSS_SELECTOR, pattern)
            if items:
                related_items = items
                print(f"        ğŸ¯ ê´€ë ¨ë‰´ìŠ¤ ì•„ì´í…œ {len(items)}ê°œ ë°œê²¬: {pattern}")
                break
        except:
            continue

    if not related_items:
        print(f"        âŒ ê´€ë ¨ë‰´ìŠ¤ ì•„ì´í…œ ì—†ìŒ")
        return 0

    # ê° ê´€ë ¨ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬
    for item in related_items[:10]:  # ìµœëŒ€ 5ê°œê¹Œì§€
        try:
            # ì œëª© ì¶”ì¶œ
            related_title = ""
            title_patterns = [
                "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1.sds-comps-text-type-body2",
                # ìƒˆë¡œìš´ íŒ¨í„´
                "span[class*='sds-comps-text-type-body2'][class*='sds-comps-text-ellipsis']",
                "span[class*='sds-comps-text'][class*='body2'][class*='ellipsis']",
                "span.sds-comps-text-type-body2",
                "span[class*='sds-comps-text-type-body2']"
            ]

            for pattern in title_patterns:
                try:
                    title_elem = item.find_element(By.CSS_SELECTOR, pattern)
                    text = title_elem.text.strip()
                    if text and len(text) > 5 and text != main_title:
                        related_title = text
                        break
                except:
                    continue

            if not related_title:
                continue

            # URL ì¶”ì¶œ
            related_naver_url = ""
            related_original_url = ""
            try:
                links = item.find_elements(By.CSS_SELECTOR, "a")
                for link in links:
                    href = link.get_attribute("href")
                    if href:
                        if "news.naver.com" in href:
                            related_naver_url = href
                        elif "media.naver.com" not in href and "n.news.naver.com" not in href:
                                related_original_url = href
            except:
                pass

            # ì–¸ë¡ ì‚¬ì™€ ë°œí–‰ì¼ (ê°„ë‹¨í•˜ê²Œ)
            related_press = ""
            related_published = ""

            try:
                # profile-info ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                profile_info = card.find_element(By.CSS_SELECTOR, ".sds-comps-profile-info")

                # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
                press_patterns = [
                    "span.sds-comps-profile-info-title-text",
                    "span[class*='profile-info-title-text']",
                    "span[class*='title-text']",
                ]

                for pattern in press_patterns:
                    try:
                        press_elem = profile_info.find_element(By.CSS_SELECTOR, pattern)
                        related_press = press_elem.text.strip()
                        if related_press:
                            break
                    except:
                        continue

                # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
                time_patterns = [
                    "span.sds-comps-profile-info-subtext",
                    "span[class*='profile-info-subtext']",
                    "span[class*='subtext']",
                ]

                for pattern in time_patterns:
                    try:
                        time_elem = profile_info.find_element(By.CSS_SELECTOR, pattern)
                        related_published = time_elem.text.strip()
                        if related_published:
                            break
                    except:
                        continue
            except:
                pass

            # ê²°ê³¼ì— ì¶”ê°€
            results.append({
                "title": related_title,
                "naver_url": related_naver_url,
                "original_url": related_original_url,
                "source": related_press,
                "published": related_published,
                "related_to": main_title,
                "related_original_url": main_original_url
            })

            added_count += 1
            print(f"        âœ… ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤ ì¶”ê°€: {related_title[:30]}...")

        except Exception as e:
            print(f"        âŒ ë‚´ë¶€ ê´€ë ¨ë‰´ìŠ¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            continue

    return added_count

def get_naver_url(card):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ URLì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        naver_container = card.find_element(By.CSS_SELECTOR, ".sds-comps-profile-info")
        naver_link = naver_container.find_element(By.CSS_SELECTOR, "a[href*='n.news.naver']")
        return naver_link.get_attribute("href")
    except:
        return ""

def get_original_url(card):
    """ì›ë³¸ ë‰´ìŠ¤ URLì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headline_span = card.find_element(By.CSS_SELECTOR,
                                        "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1.sds-comps-text-type-headline1")
        original_link = headline_span.find_element(By.XPATH, "..")  # ë¶€ëª¨ a íƒœê·¸

        # a íƒœê·¸ì´ê³  nocr="1" ì†ì„±ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
        if (original_link.tag_name.lower() == 'a' and
                original_link.get_attribute("nocr") == "1"):
            href = original_link.get_attribute("href")
            if href and "media.naver.com" not in href:
                return href
    except:
        pass
    return ""


def crawl_with_intelligent_detection(keyword, start_date_str, end_date_str):
    """ì§€ëŠ¥ì  ê°ì§€ ê¸°ëŠ¥ì´ ìˆëŠ” í¬ë¡¤ë§ í•¨ìˆ˜ (ê´€ë ¨ë‰´ìŠ¤ í¬í•¨)"""
    # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    now = datetime.now()
    date_str = now.strftime("%y%m%d")  # yymmdd í˜•ì‹
    time_str = now.strftime("%H%M")  # hhmm í˜•ì‹
    output_filename = f"naver_news_{keyword}_{date_str}_{time_str}.csv"

    options = webdriver.ChromeOptions()
    options.add_argument("--window-size=1200,900")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_experimental_option("detach", True)

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    results = []
    start_date = datetime.strptime(start_date_str, "%Y%m%d")
    end_date = datetime.strptime(end_date_str, "%Y%m%d")

    try:
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime("%Y%m%d")
            print(f"\nğŸ“… {date_str} ë‚ ì§œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

            page = 1
            while True:
                start_num = (page - 1) * 10 + 1
                url = f"https://search.naver.com/search.naver?where=news&query={keyword}&nso=so:r,p:from{date_str}to{date_str},a:all&start={start_num}"
                print(f"ğŸŒ ì ‘ì† URL: {url}")
                driver.get(url)

                # ì²« í˜ì´ì§€ì—ì„œë§Œ ë””ë²„ê¹… ì‹¤í–‰
                if current_date == start_date and page == 1:
                    debug_page_elements(driver)
                else:
                    time.sleep(2)  # ì¼ë°˜ì ì¸ ëŒ€ê¸°

                # ì§€ëŠ¥ì ìœ¼ë¡œ ë‰´ìŠ¤ ì¹´ë“œ ì°¾ê¸°
                print(f"\n=== ğŸ¯ ë‰´ìŠ¤ ì¹´ë“œ íƒì§€ ì‹œì‘ (í˜ì´ì§€ {page}) ===")
                news_cards = find_elements_intelligently(driver)

                if not news_cards or page > 3:
                    print(f"âŒ ë” ì´ìƒ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤ (í˜ì´ì§€ {page})")
                    break

                print(f"âœ… {len(news_cards)}ê°œì˜ ë‰´ìŠ¤ ì¹´ë“œ ë°œê²¬!")

                # ê° ì¹´ë“œì—ì„œ ì •ë³´ ì¶”ì¶œ
                for i, card in enumerate(news_cards):
                    print(f"\n--- ğŸ“° ë‰´ìŠ¤ {i + 1} ì²˜ë¦¬ ì¤‘ (í˜ì´ì§€ {page}) ---")

                    try:
                        # ì œëª© ì¶”ì¶œ
                        title = extract_title_intelligently(card)
                        if not title:
                            print(f"    âŒ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨")
                            continue

                        # URL ì¶”ì¶œ
                        naver_url = get_naver_url(card)
                        original_url = get_original_url(card)
                        print(f"        ë„¤ì´ë²„ URL: {naver_url}")
                        print(f"        ì›ë³¸ URL: {original_url}")

                        # ì–¸ë¡ ì‚¬ ì¶”ì¶œ (ê°„ë‹¨íˆ)
                        press = ""
                        press_patterns = [
                            "span[class*='press']",
                            "span[class*='info']",
                            "[class*='profile'] span"
                        ]
                        for pattern in press_patterns:
                            try:
                                press_elem = card.find_element(By.CSS_SELECTOR, pattern)
                                press = press_elem.text.strip()
                                if press:
                                    break
                            except:
                                continue

                        # ë°œí–‰ì¼ ì¶”ì¶œ (ê°„ë‹¨íˆ)
                        published = ""
                        date_patterns = [
                            "span[class*='date']",
                            "span[class*='time']",
                            "[class*='info'] span"
                        ]
                        for pattern in date_patterns:
                            try:
                                date_elem = card.find_element(By.CSS_SELECTOR, pattern)
                                text = date_elem.text.strip()
                                if any(x in text for x in ['ì‹œê°„', 'ë¶„', 'ì¼', 'ì›”', 'ë…„', ':']):
                                    published = text
                                    break
                            except:
                                continue



                        result = {
                            "title": title,
                            "naver_url": naver_url,
                            "original_url": original_url,
                            "source": press,
                            "published": published,
                            "related_to": "",
                            "related_original_url": ""
                        }

                        results.append(result)
                        print(f"    âœ… ì¶”ì¶œ ì™„ë£Œ: {title[:30]}... | {press} | {published}")

                        # ğŸ”„ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬
                        print(f"    ğŸ”— ê´€ë ¨ ë‰´ìŠ¤ íƒì§€ ì¤‘...")
                        related_count = process_related_news(driver, card, title, original_url, results)
                        if related_count > 0:
                            print(f"    âœ… ê´€ë ¨ ë‰´ìŠ¤ {related_count}ê±´ ì¶”ê°€")

                    except Exception as e:
                        print(f"    âŒ ë‰´ìŠ¤ {i + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue

                print(f"ğŸ“„ í˜ì´ì§€ {page} ì™„ë£Œ")
                page += 1

            print(f"ğŸ“… {date_str} ë‚ ì§œ ìˆ˜ì§‘ ì™„ë£Œ")
            current_date += timedelta(days=1)

        # ê²°ê³¼ ì €ì¥
        if results:
            with open(output_filename, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.DictWriter(f, fieldnames=[
                    "title", "naver_url", "original_url", "source", "published",
                    "related_to", "related_original_url"
                ])
                writer.writeheader()
                writer.writerows(results)

            # ê²°ê³¼ ìš”ì•½
            main_news = [r for r in results if not r['related_to']]
            related_news = [r for r in results if r['related_to']]

            print(f"\nâœ… ì´ {len(results)}ê±´ ì €ì¥ ì™„ë£Œ â†’ {output_filename}")
            print(f"   ğŸ“° ì£¼ìš” ë‰´ìŠ¤: {len(main_news)}ê±´")
            print(f"   ğŸ”— ê´€ë ¨ ë‰´ìŠ¤: {len(related_news)}ê±´")
        else:
            print("\nâŒ ì¶”ì¶œëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    finally:
        driver.quit()

    return results


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ ì§€ëŠ¥ì  ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘")


    # ë‚ ì§œ ë²”ìœ„ í…ŒìŠ¤íŠ¸ (ì˜ˆ: 6ì›” 1ì¼ ~ 6ì›” 6ì¼)
    results = crawl_with_intelligent_detection("ì¶œì‚°", "20240601", "20240601")

    if results:
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        main_news = [r for r in results if not r['related_to']]
        related_news = [r for r in results if r['related_to']]

        print(f"ğŸ“° ì£¼ìš” ë‰´ìŠ¤: {len(main_news)}ê±´")
        print(f"ğŸ”— ê´€ë ¨ ë‰´ìŠ¤: {len(related_news)}ê±´")
        print(f"ğŸ“ˆ ì´ ë‰´ìŠ¤: {len(results)}ê±´")

        # ìƒ˜í”Œ ì¶œë ¥
        for i, result in enumerate(main_news[:3]):
            print(f"{i + 1}. {result['title'][:50]}...")
    else:
        print("\nğŸ”§ í˜„ì¬ í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        print("   ìœ„ì˜ ë””ë²„ê¹… ì¶œë ¥ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ ì…€ë ‰í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")